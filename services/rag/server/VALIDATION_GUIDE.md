# RAG Validation Guide

## Quick Start

### 1. Generar PDFs Sint√©ticos

```bash
cd services/rag/server
python generate_test_pdfs.py
```

Esto crear√° 5 PDFs de prueba en `./test_pdfs/`:
- `technical_architecture.pdf` - Arquitectura del sistema
- `product_features.pdf` - Caracter√≠sticas del producto  
- `user_guide.pdf` - Gu√≠a de usuario
- `faq_document.pdf` - Preguntas frecuentes
- `research_paper.pdf` - Paper de investigaci√≥n (multi-p√°gina)

### 2. Levantar el Sistema

```bash
cd ../../..  # Volver a ra√≠z del proyecto
./scripts/start.ps1  # Windows
# o
./scripts/start.sh   # Linux/Mac
```

Aseg√∫rate de configurar las variables de entorno en `.env`:
```env
# Embeddings (OpenAI o local)
EMBEDDING_PROVIDER=openai  # o "local"
OPENAI_API_KEY=tu-api-key  # si usas OpenAI
# LOCAL_EMBEDDING_MODEL=BAAI/bge-m3  # si usas local

# Habilitar contenido y reranking
ENABLE_CONTENT_EMBED=1
ENABLE_RERANKING=1

# Chunking ilimitado
MAX_CHUNKS=None
CHUNK_SIZE_CHARS=600
```

### 3. Ejecutar Validaci√≥n

```bash
cd services/rag/server
python validate_rag.py
```

El script autom√°ticamente:
1. ‚úÖ Sube los 5 PDFs al servidor RAG
2. ‚úÖ Espera a que se complete la indexaci√≥n
3. ‚úÖ Ejecuta 15 consultas de prueba SIN reranking
4. ‚úÖ Ejecuta las mismas 15 consultas CON reranking
5. ‚úÖ Compara los resultados y genera un reporte

### 4. Revisar Resultados

El script genera:
- **Salida en consola**: Resumen con estad√≠sticas clave
- **Archivo JSON**: `rag_test_results_YYYYMMDD_HHMMSS.json` con resultados detallados

## Ejemplo de Salida

```
üìä RAG VALIDATION REPORT
================================================

Total Queries: 15
Results Reordered by Reranking: 8 (53.3%)

Average Latency:
  Without Reranking: 245.3ms
  With Reranking:    487.6ms
  Reranking Overhead: 242.3ms

Results by Category:
  factual_retrieval: 2/3 reordered (67%)
  technical_detail: 1/2 reordered (50%)
  research_finding: 2/2 reordered (100%)
  ...

Top 5 Reranking Impacts:
  1. What is reranking and why is it useful?
     Score change: 0.721 ‚Üí 0.912 (Œî+0.191)
  ...
```

## M√©tricas Clave a Observar

### ‚úÖ Indicadores de √âxito

1. **Reordenamiento**: 40-60% de queries deber√≠an mostrar reordenamiento
   - Indica que el reranking est√° mejorando la relevancia

2. **Latencia Aceptable**: 
   - Sin reranking: < 500ms
   - Con reranking: < 1000ms (overhead ~200-500ms)

3. **Modo de B√∫squeda**: 
   - Debe mostrar `"vector"` o `"vector+rerank"`
   - Si muestra `"like"` = problema con embeddings/Qdrant

4. **Chunks Generados**:
   - Revisar logs del servidor durante upload
   - Deber√≠as ver "Indexed N content chunks" con N >> 4

### ‚ö†Ô∏è Problemas Comunes

**Problema**: `"mode": "like"` en vez de `"vector"`
- **Causa**: Embeddings no configurados o Qdrant no disponible
- **Soluci√≥n**: Verificar `ENABLE_CONTENT_EMBED=1` y `EMBEDDING_PROVIDER`

**Problema**: Latencia muy alta (> 2 segundos)
- **Causa**: Modelo de reranking muy grande o CPU lento
- **Soluci√≥n**: Usar modelo m√°s ligero o ajustar `RERANK_TOP_K`

**Problema**: Todos los scores muy bajos (< 0.5)
- **Causa**: Embeddings no coinciden con el contenido
- **Soluci√≥n**: Reindexar documentos con la configuraci√≥n correcta

## Validaci√≥n Manual

### Consultas de Prueba Recomendadas

```python
# Factual simple
"What port does the Gateway run on?"
# Esperado: "8088" de technical_architecture.pdf

# Explicaci√≥n conceptual  
"What is reranking and why is it useful?"
# Esperado: Explicaci√≥n detallada de FAQ

# Dato cuantitativo
"What was the accuracy improvement from reranking?"
# Esperado: "15-20%" de research_paper.pdf

# Multi-documento
"What file formats are supported?"
# Esperado: Puede venir de FAQ o product_features
```

### Probar Reranking Manualmente

```bash
# Sin reranking
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"text": "What is reranking?", "limit": 5, "rerank": false}'

# Con reranking
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"text": "What is reranking?", "limit": 5, "rerank": true}'
```

Compara:
- El orden de los resultados
- Los campos `rerank_score` vs `original_score`
- La relevancia del primer resultado

## Validaci√≥n de Chunking Ilimitado

### Ver Chunks en Qdrant

```bash
# Listar colecciones
curl http://localhost:6333/collections

# Ver puntos en la colecci√≥n de contenido
curl http://localhost:6333/collections/uploads-content/points/scroll?limit=100
```

Deber√≠as ver:
- Muchos m√°s puntos que antes (100-200 por documento vs 4)
- Payload con `chunk_index`, `total_chunks`, `chunk_type`

### Comparaci√≥n Antes/Despu√©s

**Antes (MAX_CHUNKS=4)**:
- PDF 50 p√°ginas ‚Üí 4 chunks
- Mucho contenido perdido
- B√∫squedas fallan en contenido intermedio

**Despu√©s (MAX_CHUNKS=None)**:
- PDF 50 p√°ginas ‚Üí ~100-150 chunks
- Todo el contenido indexado
- B√∫squedas encuentran cualquier secci√≥n

## Tips de Optimizaci√≥n

### Si usas OpenAI API
```env
# Reducir costos manteniendo calidad
CHUNK_SIZE_CHARS=600  # Chunks m√°s peque√±os = menos tokens
MAX_CHUNKS=100        # L√≠mite razonable en vez de None
```

### Si usas modelos locales
```env
# M√°xima calidad sin coste
EMBEDDING_PROVIDER=local
LOCAL_EMBEDDING_MODEL=BAAI/bge-m3  # Mejor calidad multiling√ºe
MAX_CHUNKS=None                     # Sin l√≠mites
ENABLE_RERANKING=1                  # Reranking siempre
```

### Ajuste fino de reranking
```env
# Para latencia sensible
RERANK_TOP_K=20   # Menos resultados a rerank
RERANK_FINAL_K=5  # Menos resultados finales

# Para m√°xima calidad
RERANK_TOP_K=100  # M√°s resultados a rerank
RERANK_FINAL_K=10 # M√°s resultados finales
```

## Siguientes Pasos

Despu√©s de validar con PDFs sint√©ticos:

1. **Probar con documentos reales** de tu dominio
2. **Ajustar par√°metros** seg√∫n los resultados
3. **Crear tu propio test_queries.json** con preguntas de tu caso de uso
4. **Monitorear m√©tricas** en producci√≥n con Prometheus

---

## Estructura de Archivos de Validaci√≥n

```
services/rag/server/
‚îú‚îÄ‚îÄ generate_test_pdfs.py      # Generador de PDFs
‚îú‚îÄ‚îÄ test_queries.json           # Consultas de prueba
‚îú‚îÄ‚îÄ validate_rag.py             # Script de validaci√≥n
‚îú‚îÄ‚îÄ test_pdfs/                  # PDFs generados
‚îÇ   ‚îú‚îÄ‚îÄ technical_architecture.pdf
‚îÇ   ‚îú‚îÄ‚îÄ product_features.pdf
‚îÇ   ‚îú‚îÄ‚îÄ user_guide.pdf
‚îÇ   ‚îú‚îÄ‚îÄ faq_document.pdf
‚îÇ   ‚îî‚îÄ‚îÄ research_paper.pdf
‚îî‚îÄ‚îÄ rag_test_results_*.json    # Resultados generados
```
